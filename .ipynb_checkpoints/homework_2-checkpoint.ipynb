{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Homework 2\n",
    "\n",
    "Due: Tuesday March 1, 11:59pm\n",
    "\n",
    "Late deadline (50\\% off): Thursday March 3, 11:59pm\n",
    "\n",
    "Name: Dani Song\n",
    "\n",
    "NetID: ds2288\n",
    "\n",
    "\n",
    "## Instructions\n",
    "Read through this python notebook. You will be asked questions and given programming tasks. These are numbered, followed by point values for grading (for example, \"Question 17.8 (5pt)\"). You should edit the notebook directly to input your solution code and question answers. You can double click on text to edit it, which you should use for putting your question answers in the notebook.\n",
    "\n",
    "When you are ready to submit, you should run all of your code (click Cell->Run All). Then you should submit **three** files to gradescope:\n",
    "* PDF: export to PDF (click File->Download as->PDF via LaTeX)\n",
    "* .py: export to Python (click File->Download as->Python (.py))\n",
    "* .ipynb: submit the edited Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Probabilistic Routines\n",
    "You will be using the following helper functions for manipulating probability distributions.\n",
    "\n",
    "Recall from lecture that you should represent probabilities as the logarithm of the probability. Whenever you want to multiply probabilities, instead you just add the log probabilities. Whenever you want to add probabilities, instead you just `log_sum_exp` the log probabilities. Whenever you want to add a list of probabilities, instead you just `log_sum_exp_list` the list of log probabilities.\n",
    "\n",
    "Generally in this homework we will be representing distributions over a random variable $X$ by dictionary mapping $x$ to $\\log P(X=x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, isinf\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "\n",
    "def log(x):\n",
    "    if x==0.:\n",
    "        return float(\"-inf\")\n",
    "    return math.log(x)\n",
    "\n",
    "def sample_from_log_distribution(distribution):\n",
    "    \"\"\"\n",
    "    distribution: a dictionary mapping x to log P(x)\n",
    "    stochastically draws a sample from the distribution P(x)\n",
    "    \"\"\"\n",
    "    distribution = list(distribution.items())\n",
    "    values = [v for v, _ in distribution ]\n",
    "    probabilities = np.array([exp(log_prob) for _, log_prob in distribution ])\n",
    "    result = np.random.multinomial(1, probabilities)\n",
    "    for v, r in zip(values, result):\n",
    "        if r:\n",
    "            return v\n",
    "\n",
    "    assert False\n",
    "    \n",
    "def log_sum_exp(first_log_probability, second_log_probability):\n",
    "    \"\"\"\n",
    "    Returns log(exp(first_log_probability) + exp(second_log_probability))\n",
    "    Does so in a numerically stable way\n",
    "    \"\"\"\n",
    "    if isinf(first_log_probability) and first_log_probability<0:\n",
    "        return second_log_probability\n",
    "    if isinf(second_log_probability) and second_log_probability<0:\n",
    "        return first_log_probability\n",
    "    larger, smaller = max(first_log_probability, second_log_probability), min(first_log_probability, second_log_probability)\n",
    "    return larger + log(1 + exp(smaller-larger))\n",
    "\n",
    "def log_sum_exp_list(lst):\n",
    "    \"\"\"\n",
    "    Returns log(sum(exp(log_probability) for log_probability in lst))\n",
    "    Does so in a numerically stable way\n",
    "    \"\"\"\n",
    "    total=float(\"-inf\")\n",
    "    for log_probability in lst:\n",
    "        total = log_sum_exp(total, log_probability)\n",
    "    return total\n",
    "\n",
    "def norm_log_distribution(distribution):\n",
    "    \"\"\"\n",
    "    distribution is a dictionary mapping x to log (alpha * P(x))\n",
    "    where alpha is a normalization constant\n",
    "    returns a dictionary mapping x to log P(x), ie a normalized distribution\n",
    "    Does so in a numerically stable way\n",
    "    \"\"\"\n",
    "    normalizer = log_sum_exp_list(list(distribution.values()))\n",
    "    return {x: log_probability - normalizer\n",
    "            for x, log_probability in distribution.items()}\n",
    "\n",
    "def expectation(distribution, f):\n",
    "    \"\"\"\n",
    "    f is a function from x to a real number\n",
    "    distribution is a dictionary mapping x to log P(x)\n",
    "    returns the expectation of f under the distribution P\n",
    "    \"\"\"\n",
    "    return sum( f(x)*exp(log_probability)\n",
    "                for x, log_probability in norm_log_distribution(distribution).items() )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alarm Network\n",
    "You will be implementing and experimenting with the alarm network given in class, which is diagrammed below.\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/profile/Alireza-Khanteymoori/publication/278673563/figure/fig1/AS:294244133687301@1447164676400/An-example-of-a-Bayesian-network.png\" width=\"500\">\n",
    "</center>\n",
    "\n",
    "First we represent the distribution over burglary with a dictionary mapping values of the burglary random variable to the log probability of that burglary value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "burglary_distribution = {True: log(0.001), False: log(1-0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we do the same thing for earthquake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_distribution = {True: log(0.002), False: log(1-0.002)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because alarm is directly caused by burglary and earthquake, we represent its conditional distribution as a function that inputs burglary and earthquake. The `alarm_distribution` function returns the probability distribution over whether the alarm goes off, given those inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alarm_distribution(burglary, earthquake):\n",
    "    if burglary and earthquake: return {True: log(0.95), False: log(1-0.95)}\n",
    "    if burglary and not earthquake: return {True: log(0.94), False: log(1-0.94)}\n",
    "    if not burglary and earthquake: return {True: log(0.29), False: log(1-0.29)}\n",
    "    if not burglary and not earthquake: return {True: log(0.001), False: log(1-0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Defining the rest of the network (1pt)\n",
    "Implement `mary_distribution` and `john_distribution`, which should be functions that take the setting of `alarm` (either `True` or `False`) and which return a dictionary mapping the setting of MaryCalls/JohnCalls (either `True` or `False`) to a log probability. This should be analogous to `alarm_distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mary_distribution(alarm):\n",
    "    if alarm: return {True: log(0.70), False: log(1-0.70)}\n",
    "    if not alarm: return {True: log(0.01), False: log(1-0.01)}\n",
    "def john_distribution(alarm):\n",
    "    if alarm: return {True: log(0.90), False: log(1-0.90)}\n",
    "    if not alarm: return {True: log(0.05), False: log(1-0.05)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 Defining the joint distribution (2pt)\n",
    "Implement `log_joint`. Its arguments are settings of the random variables (`burglary`, `earthquake`, `alarm`, `mary`, `john`). It should return the logarithm of the joint probability of that variable assignment: \n",
    "$$\\log P(\\text{Burglary=burglary, Earthquake=earthquake, Alarm=alarm, MaryCalls=mary, JohnCalls=john})$$\n",
    "\n",
    "Your implementation of `log_joint` should use `burglary_distribution`, `earthquake_distribution`, `alarm_distribution`, `mary_distribution`, and `john_distribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_joint(burglary, earthquake, alarm, mary, john):\n",
    "    return burglary_distribution[burglary] + earthquake_distribution[earthquake] + alarm_distribution(burglary, earthquake)[alarm] + mary_distribution(alarm)[mary] + john_distribution(alarm)[john]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 Calculate $\\log P(\\text{Mary=True, John=True})$ (2pt)\n",
    "Using `log_sum_exp_list` and `log_joint`, calculate $\\log P(\\text{Mary=True, John=True})$.\n",
    "Store this calculation result in the variable `mary_and_john`.\n",
    "\n",
    "Hint: you might want to try a *list comprehension* that looks like:\n",
    "\n",
    "`[ ... for burglary in [True, False] for earthquake in [True, False] for alarm in [True, False] ]`\n",
    "\n",
    "Alternatively you could use three nested `for` loops to iterate over all possible variable assignments to earthquake, burglary, and alarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(John=True, Mary=True) = 0.0020841002390000014\n"
     ]
    }
   ],
   "source": [
    "mary_and_john = log_sum_exp_list([log_joint(burglary, earthquake, alarm, True, True) for burglary in [True, False] for earthquake in [True, False] for alarm in [True, False]])\n",
    "print(\"P(John=True, Mary=True) =\", exp(mary_and_john))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4 More calculations (2pt)\n",
    "Similarly, calculate $\\log P(\\text{Burglar=True, Mary=True, John=True})$ and $\\log P(\\text{Burglar=False, Mary=True, John=True})$. Put these quantities into the variables `burglary_and_mary_and_john` and `not_burglary_and_mary_and_john`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(Burglar=True, John=True, Mary=True) = -7.4315942266119235\n",
      "log P(Burglar=False, John=True, Mary=True) = -6.507733191269771\n"
     ]
    }
   ],
   "source": [
    "burglary_and_mary_and_john = log_sum_exp_list([log_joint(True, earthquake, alarm, True, True) for earthquake in [True, False] for alarm in [True, False]])\n",
    "not_burglary_and_mary_and_john = log_sum_exp_list([log_joint(False, earthquake, alarm, True, True) for earthquake in [True, False] for alarm in [True, False]])\n",
    "print(\"log P(Burglar=True, John=True, Mary=True) =\", burglary_and_mary_and_john)\n",
    "print(\"log P(Burglar=False, John=True, Mary=True) =\", not_burglary_and_mary_and_john)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.5 Calculate posterior $P(\\text{Burglar=True}|\\text{Mary=True, John=True})$ (1pt)\n",
    "Write down a single line of code which will calculate and print out $P(\\text{Burglar=True}|\\text{Mary=True, John=True})$. You should reuse `burglary_and_mary_and_john` and `mary_and_john`.\n",
    "Hint: Use Bayes rule to relate $P(\\text{Burglar=True}|\\text{Mary=True, John=True})$ to $P(\\text{Burglar=True}, \\text{Mary=True, John=True})$ and $P(\\text{Mary=True, John=True})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.4315942266119235\n",
      "P(Burglar=True|John=True, Mary=True) = 0.2841718353643928\n"
     ]
    }
   ],
   "source": [
    "print(burglary_and_mary_and_john)\n",
    "print(\"P(Burglar=True|John=True, Mary=True) =\", exp(burglary_and_mary_and_john - mary_and_john)) # implement as part of homework; fill in None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.6 Another calculation for $P(\\text{Burglar=True}|\\text{Mary=True, John=True})$ (2pt)\n",
    "Construct an unnormalized distribution over Burglar, conditioned on both Mary and John calling.\n",
    "Store this inside the variable `unnormalized_log_posterior`, which should be a dictionary. The dictionary entry `unnormalized_log_posterior[b]` should be the logarithm of a probability that is proportional to $P(\\text{Burglar=b} | \\text{Mary=True, John=True})$.\n",
    "\n",
    "Normalize this distribution (`unnormalized_log_posterior`) by calling `norm_log_distribution`, and store the result inside a new variable called `log_posterior`.\n",
    "The dictionary entry `log_posterior[b]` should be the logarithm of $P(\\text{Burglar=b}| \\text{Mary=True, John=True})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log P(Burglar|John=True, Mary=True) = {True: -1.258176169692387, False: -0.3343151343502342}\n",
      "P(Burglar=True|John=True, Mary=True) = 0.2841718353643928\n"
     ]
    }
   ],
   "source": [
    "unnormalized_log_posterior = {True: burglary_and_mary_and_john, False: not_burglary_and_mary_and_john} # implement as part of homework\n",
    "log_posterior = norm_log_distribution(unnormalized_log_posterior) # implement as part of homework\n",
    "print(\"log P(Burglar|John=True, Mary=True) =\", log_posterior)\n",
    "print(\"P(Burglar=True|John=True, Mary=True) =\", exp(log_posterior[True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that you have now calculated $P(\\text{Burglar=True}|\\text{Mary=True, John=True})$ twice, in two different ways. You should get the same result both times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Away\n",
    "\n",
    "### Task 1.7 Programming (1pt)\n",
    "Calculate and print out $P(\\text{Burglar=True}|\\text{Mary=True, John=True, Earthquake=True})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Burglar=True|Mary=True, John=True, Earthquake=True) = 0.0032622416021499314\n"
     ]
    }
   ],
   "source": [
    "mary_and_john_and_earthquake = log_sum_exp_list([log_joint(burglary, True, alarm, True, True) for burglary in [True, False] for alarm in [True, False]])\n",
    "burglary_and_mary_and_john_and_earthquake = log_sum_exp_list([log_joint(True, True, alarm, True, True) for alarm in [True, False]])\n",
    "print(\"P(Burglar=True|Mary=True, John=True, Earthquake=True) =\", \n",
    "      exp(burglary_and_mary_and_john_and_earthquake - mary_and_john_and_earthquake)) # implement as part of homework; fill in None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.8 (1pt)\n",
    "Why did the probability of burglary go down given Mary and John calling, after we conditioned on the earthquake happening?\n",
    "\n",
    "* **Solution: Because the probability of burglary happening simultenously to an earthquake is lower than burglary or earthquake happening alone.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model\n",
    "\n",
    "You will be implementing and experimenting with the Hidden Markov Model (HMM) given in class, which is diagrammed below.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.cs.cornell.edu/~ellisk/res/hmm.png\" width=\"800\">\n",
    "\n",
    "</center>\n",
    "\n",
    "The next Jupyter Notebook cell contains an implementation of Hidden Markov Model in the class `HMM`. You will be filling out parts of this implementation.\n",
    "\n",
    "## Task 2.1 Simulating from the Bayesian Network (5pt)\n",
    "Implement `sample_from_model`. You should use the method `sample_observation_given_state` for drawing from $P(X_t|Z_t)$, and the function `sample_from_log_distribution` for drawing from $P(Z_{t+1}|Z_{t})$ and for drawing from $P(Z_{1})$.\n",
    "\n",
    "**You should first sample from $P(Z_{1})$, then $P(X_1|Z_1)$, then $P(Z_{2}|Z_{1})$, then $P(X_2|Z_2)$, then $P(Z_{3}|Z_{2})$, then $P(X_{3}|Z_{3})$, etc.** You could sample in other orders, but please do it in this order so that your results are identical to the reference results.\n",
    " \n",
    "\n",
    "## Task 2.2 Calculating the forward distribution $P(Z_t|X_{1:t})$ (10pt)\n",
    "\n",
    "Implement the forward algorithm for calculating $P(Z_t|X_{1:t})$. Do so within the method `forward`, and implement the API described in the docstring (*remember to calculate log probabilities, and your solution should linear time*)\n",
    "\n",
    "## Task 2.3 Calculating the backward distribution $P(X_{t+1:T}|Z_t)$ (10pt)\n",
    "\n",
    "Implement the backward algorithm for calculating $P(X_{t+1:T}|Z_t)$. Do so within the method `backward`, and implement the API described in the docstring (*remember to calculate log probabilities, and your solution should linear time*)\n",
    "\n",
    "## Task 2.4 Calculating the forward/backward (smoothing) distribution $P(Z_t|X_{1:T})$ (5pt)\n",
    "\n",
    "Implement the forward/backward algorithm for calculating $P(Z_t|X_{1:T})$. Do so within the method `forward_backward`, and implement the API described in the docstring (*remember to calculate log probabilities, and your solution should linear time*)\n",
    "\n",
    "## Calculating the marginal likelihood $P(X_{1:T})$\n",
    "\n",
    "### Task 2.5.1  (10pt)\n",
    "Derive an expression for $P(X_{1:T})$ in terms of $P(Z_1)$, $P(X_1|Z_1)$, and the \"backward\" distribution $P(X_{2:T} | Z_1)$.\n",
    "Write down the resulting equation for $P(X_{1:T})$.\n",
    "\n",
    "**Your equation here (doubleclick on Jupyter notebook box to edit):** \n",
    "* $$P(X_{1:T})= P(Z_1)P(X_1|Z_1)P(X_{2:T} | Z_1)$$\n",
    "\n",
    "### Task 2.5.2 (5pt)\n",
    "Implement the `marginal_likelihood` method of `HMM`, which calculates $\\log P(X_{1:T})$. You should use the equation that you derived in the previous part, together with your implementation of `backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, state_space, transition_distribution, initial_state_distribution):\n",
    "        \"\"\"\n",
    "        state_space: list of possible values that each hidden state (Z_t) can assume\n",
    "        transition_distribution: representation of log P(Z_{t+1} | Z_t).\n",
    "           Should be a dictionary of dictionaries where:\n",
    "           `transition_distribution[state_t][state_{t+1}] = log P(Z_{t+1}=state_{t+1} | Z_{t}=state_t)`\n",
    "        initial_state_distribution: representation of log P(Z_1).\n",
    "           Should be a dictionary where `initial_state_distribution[state_1] = log P(Z_1=state_1)`\n",
    "        \"\"\"\n",
    "        self.state_space, self.transition_distribution, self.initial_state_distribution =\\\n",
    "             state_space, transition_distribution, initial_state_distribution\n",
    "\n",
    "        normalizing_constant = log_sum_exp_list(list(initial_state_distribution.values()))\n",
    "        if abs(normalizing_constant)>1e-5:\n",
    "            assert False, f\"Your initial state distribution is not normalized. Remembered that it should contain log probabilities.\"\n",
    "\n",
    "        for s in state_space:\n",
    "            normalizing_constant = log_sum_exp_list([self.transition_distribution[s][next_state]\n",
    "                                                     for next_state in state_space])\n",
    "            if abs(normalizing_constant)>1e-5:\n",
    "                assert False, f\"Your transition distribution is not normalized. Remembered that it should contain log probabilities. It is not normalized for state {s}\"\n",
    "        \n",
    "\n",
    "        self.forward_dynamic_programming = {}\n",
    "        self.backward_dynamic_programming = {}\n",
    "        \n",
    "    def logProb_observation_given_state(self, observation, state):\n",
    "        \"\"\"\n",
    "        calculates log P(X_t=observation | Z_t=state)\n",
    "        \"\"\"\n",
    "        assert False, \"should be defined by the class inheriting from HMM\"\n",
    "\n",
    "    def sample_observation_given_state(self, state):\n",
    "        \"\"\"\n",
    "        stochastically draws a sample from P(X_t | Z_t=state)\n",
    "        \"\"\"\n",
    "        assert False, \"should be defined by the class inheriting from HMM\"\n",
    "\n",
    "    def sample_from_model(self, time_steps):\n",
    "        \"\"\"\n",
    "        Returns a sample from the generative model, not conditioned on anything\n",
    "        The sample consists of both a list of observations and a list of hidden states\n",
    "        Both of these lists should have length `time_steps`\n",
    "        Return the pair (Z_{1:T}, X_{1:T})\n",
    "        \n",
    "        You should call out to the method `sample_observation_given_state`, \n",
    "          which will be implemented by classes inheriting from HMM\n",
    "          \n",
    "        ***You should first sample from $P(Z_{1})$, then $P(X_1|Z_1)$,\n",
    "                                   then $P(Z_{2}|Z_{1})$, then $P(X_2|Z_2)$, \n",
    "                                   then $P(Z_{3}|Z_{2})$, then $P(X_{3}|Z_{3})$, \n",
    "                                   etc.\n",
    "        You could sample in other orders, but please do it in this order!\n",
    "        This ensures that your results are identical to the reference results.***\n",
    "        \"\"\"\n",
    "        state = []\n",
    "        observation = []\n",
    "        \n",
    "        prev_state = sample_from_log_distribution(self.initial_state_distribution)\n",
    "        prev_observation = self.sample_observation_given_state(prev_state)\n",
    "        state.append(prev_state)\n",
    "        observation.append(prev_observation)\n",
    "        \n",
    "        for i in range(time_steps - 1):\n",
    "            curr_state = sample_from_log_distribution(self.transition_distribution[prev_state])\n",
    "            curr_observation = self.sample_observation_given_state(curr_state)\n",
    "            state.append(curr_state)\n",
    "            observation.append(curr_observation)\n",
    "            prev_state = curr_state\n",
    "        \n",
    "        return (state, observation)\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        \"\"\"\n",
    "        Returns log P(Z_t=s | X_{1:t}), where observations is a list containing X_{1:t} and t=len(observations)\n",
    "        This distribution is returned by a dictionary mapping `s` to log P(Z_t=s | X_{1:t})\n",
    "        If you do your dynamic programming via memoization+recursion, please use `self.forward_dynamic_programming`\n",
    "        \"\"\"        \n",
    "        for i in range(len(observations)):\n",
    "            if (i == 0):\n",
    "                initial_forward = {}\n",
    "                for i in self.state_space:\n",
    "                    initial_forward[i] = self.initial_state_distribution[i] + self.logProb_observation_given_state(observations[0], i)\n",
    "                self.forward_dynamic_programming[0] = initial_forward\n",
    "            else:\n",
    "                later_forwards = {}\n",
    "                for s in self.state_space:\n",
    "                    sum_probabilities = float(\"-inf\")\n",
    "                    for ss in self.state_space:\n",
    "                        sum_probabilities = log_sum_exp(sum_probabilities, self.logProb_observation_given_state(observations[i], s) + self.transition_distribution[ss][s] + self.forward_dynamic_programming[i-1][ss])\n",
    "                    later_forwards[s] = sum_probabilities\n",
    "                self.forward_dynamic_programming[i] = later_forwards\n",
    "        return norm_log_distribution(self.forward_dynamic_programming[len(observations) - 1])\n",
    "\n",
    "    def backward(self, observations):\n",
    "        \"\"\"\n",
    "        Returns `log P(X_{t+1:T} | Z_t=s)`, for each state `s`,\n",
    "           where `observations` is a list containing X_{t+1:T}, and observations[i]=X_{i+1+t}\n",
    "        Concretely, return a dictionary mapping `s` to log P(X_{t+1:T} | Z_t=s)\n",
    "        If you do your dynamic programming via memoization+recursion, please use `self.backward_dynamic_programming`\n",
    "        (Note that this is not a distribution over Z_t--what you return should not necessarily be normalized)\n",
    "        \"\"\"\n",
    "        for i in range(len(observations)):\n",
    "            if (i == 0):\n",
    "                initial_backward = {}\n",
    "                for i in self.state_space:\n",
    "                    initial_backward[i] = 1\n",
    "                self.backward_dynamic_programming[len(observations) - 1] = initial_backward\n",
    "            else:\n",
    "                later_backwards = {}\n",
    "                for s in self.state_space:\n",
    "                    sum_probabilities = float(\"-inf\")\n",
    "                    for ss in self.state_space:\n",
    "                        sum_probabilities = log_sum_exp(sum_probabilities, self.backward_dynamic_programming[len(observations) - i][ss] + self.logProb_observation_given_state(observations[len(observations) - 1 - i], ss) + self.transition_distribution[s][ss])\n",
    "                    later_backwards[s] = sum_probabilities\n",
    "                self.backward_dynamic_programming[len(observations) - 1 - i] = later_backwards\n",
    "        return norm_log_distribution(self.backward_dynamic_programming[0])\n",
    "       \n",
    "    def forward_backward(self, observations, time):\n",
    "        \"\"\"\n",
    "        Returns log P(Z_t | X_{1:T}), where t is one-indexed (starts counting at 1)\n",
    "        The input variable `time` is t\n",
    "        The list `observations` is equal to X_{1:T}\n",
    "        So, 1<=time<=len(observations)\n",
    "        The returned distribution should be represented by a dictionary mapping state `s` to log P(Z_t=s | X_{1:T})\n",
    "        \n",
    "        A Python array slicing tip:\n",
    "           observations[:time] will extract the first `time` elements of `observations`\n",
    "           observations[time:] will drop the first `time` elements of `observations`\n",
    "        These satisfy:\n",
    "           observations == observations[:time]+observations[time:]\n",
    "        \n",
    "        This should be a straightforward method to implement:\n",
    "           it should just call your forward and backward methods and do a small amount of computation after that\n",
    "        \"\"\"\n",
    "        forward_backwards = {}\n",
    "        \n",
    "#         if (len(observations[:time]) == 0):\n",
    "#             forwards = {}\n",
    "#             for s in self.state_space:\n",
    "#                 forwards[s] = float(\"-inf\")\n",
    "#         else:\n",
    "#             forwards = self.forward(observations[:time])\n",
    "            \n",
    "#         if (len(observations[time:]) == 0):\n",
    "#             backwards = {}\n",
    "#             for s in self.state_space:\n",
    "#                 backwards[s] = float(\"-inf\")\n",
    "#         else:\n",
    "#             backwards = self.backward(observations[time:])        \n",
    "        if (len(observations[:time]) == 0):\n",
    "            backwards = self.backward(observations[time:])\n",
    "            forward_backwards = backwards\n",
    "        elif (len(observations[time:]) == 0):\n",
    "            forwards = self.forward(observations[:time])\n",
    "            forward_backwards = forwards\n",
    "        else:\n",
    "            for s in self.state_space:\n",
    "                forward_backwards[s] = forwards[s] + backwards[s]\n",
    "            \n",
    "        return norm_log_distribution(forward_backwards)\n",
    "\n",
    "    def marginal_likelihood(self, observations):\n",
    "        \"\"\"\n",
    "        Returns log P(observations)\n",
    "        \"\"\"\n",
    "        backwards = self.backward(observations)\n",
    "        sum_probabilities = float(\"-inf\")\n",
    "        for s in self.state_space:\n",
    "            sum_probabilities = log_sum_exp(sum_probabilities, backwards[s] + self.initial_state_distribution[s] + self.logProb_observation_given_state(observations[0], s))\n",
    "        return sum_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Spatial Tracker Model\n",
    "\n",
    "You will be working with a HMM for spatial 2D tracking on a grid world. The grid has dimensions $\\text{size}\\times\\text{size}$.\n",
    "\n",
    "The hidden state at each time point is the true location of the moving object:\n",
    "$$Z_t\\in \\{(x, y) \\,|\\,x\\in 0...(\\text{size}-1), \\,y\\in 0...(\\text{size}-1)\\}$$\n",
    "\n",
    "The observation at each time point is the reading of a sensor that reports location in the same 2D grid world:\n",
    "$$X_t\\in \\{(x, y) \\,|\\,x\\in 0...(\\text{size}-1), \\,y\\in 0...(\\text{size}-1)\\}$$\n",
    "\n",
    "With probability $(1-\\epsilon)$, the sensor reports a grid location close to the true location. This closeness is controlled by a parameter called $\\alpha$.\n",
    "With probability $\\epsilon$, the sensor reports a \"garbage\" sensor reading at a random grid location:\n",
    "$$P(X_t=(x_1, y_1)\\,|\\,Z_t=(x_2, y_2)) = (1-\\epsilon)\\frac{\\exp(-\\alpha((x_1-x_2)^2+(y_1-y_2)^2))}{\\sum_{x_3=0}^{\\text{size}-1}\\sum_{y_3=0}^{\\text{size}-1}\\exp(-\\alpha((x_3-x_2)^2+(y_3-y_2)^2))}+\\epsilon \\frac{1}{\\text{size}^2}$$\n",
    "\n",
    "The moving object tends to stay close to where it was before, and this tendency is controlled by a parameter called $\\beta$:\n",
    "$$P(Z_{t+1}=(x, y)\\,|\\,Z_t=(x', y')) \\propto  \\exp(-\\beta((x-x')^2+(y-y')^2))$$\n",
    "Notice the above equation holds only up to a constant of proportionality.\n",
    "\n",
    "Apriori, the moving object is distributed uniformly at random at the initial time step:\n",
    "$$P(Z_1)=\\frac{1}{\\text{size}^2}$$\n",
    "\n",
    "## Task 3.1 Implement the constructor (5pt)\n",
    "Implement the constructor of `Tracker`. You should calculate and set `transition_distribution`, `self.observation_distribution`, and `initial_distribution`.\n",
    "\n",
    "## Task 3.2 Implement `logProb_observation_given_state` (2pt)\n",
    "Implement the method `logProb_observation_given_state` using `self.observation_distribution`. It should only be one line.\n",
    "\n",
    "## Task 3.3 Implement `sample_observation_given_state` (2pt)\n",
    "Implement the method `sample_observation_given_state` using `sample_from_log_distribution` and `self.observation_distribution`. It should only be one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker(HMM):\n",
    "    def __init__(self, size, alpha, beta, epsilon):\n",
    "        state_space = [(x,y) for x in range(size) for y in range(size) ]\n",
    "        observation_space = state_space\n",
    "        \n",
    "        # Populate `transition_distribution` so that transition_distribution[s1][s2]=log P(Z_{t+1}=s2|Z_t=s1)\n",
    "        transition_distribution = {}\n",
    "        \n",
    "        for z in state_space:\n",
    "            transition_distribution[z] = {}\n",
    "            for z_next in state_space:\n",
    "                transition_distribution[z][z_next] = log(exp(-beta*((z[0] - z_next[0])**2 + (z[1] - z_next[1])**2)))\n",
    "            transition_distribution[z] = norm_log_distribution(transition_distribution[z])\n",
    "                \n",
    "        # Populate `self.observation_distribution` so that self.observation_distribution[z][x]=log P(X_t=x|Z_t=z)  \n",
    "        self.observation_distribution = {}\n",
    "        \n",
    "        for z in state_space:\n",
    "            self.observation_distribution[z] = {}\n",
    "            denominator = 0\n",
    "            for xx in range(size):\n",
    "                for yy in range(size):\n",
    "                    denominator += exp(-alpha*((xx - z[0])**2 + (yy - z[1])**2))\n",
    "            for x in observation_space:\n",
    "                self.observation_distribution[z][x] = log((1-epsilon)*(exp(-alpha*((x[0] - z[0])**2 + (x[1] - z[1])**2)) / denominator) + epsilon * (1/(size**2)))\n",
    "            self.observation_distribution[z] = norm_log_distribution(self.observation_distribution[z])\n",
    "            \n",
    "        # Populate `initial_state_distribution` such that initial_state_distribution[z]=log P(Z_1=z)  \n",
    "        initial_state_distribution = {}\n",
    "        \n",
    "        for z in state_space:\n",
    "            initial_state_distribution[z] = log(1/(size**2))\n",
    "\n",
    "        for z in state_space:\n",
    "            normalizing_constant = log_sum_exp_list([self.observation_distribution[z][x]\n",
    "                                                     for x in observation_space])\n",
    "            if abs(normalizing_constant)>1e-5:\n",
    "                assert False, f\"Your observation distribution is not normalized. Remembered that it should contain log probabilities. It is not normalized for state {z}\"\n",
    "\n",
    "        super().__init__(state_space, transition_distribution, initial_state_distribution)\n",
    "\n",
    "    def logProb_observation_given_state(self, observation, state):\n",
    "        \"\"\"\n",
    "        calculates log P(observation_t | state_t)\n",
    "        \"\"\"\n",
    "        return self.observation_distribution[state][observation]\n",
    "\n",
    "    def sample_observation_given_state(self, state):\n",
    "        \"\"\"\n",
    "        stochastically draws a sample from P(observation_t | state_t)\n",
    "        \"\"\"\n",
    "        return sample_from_log_distribution(self.observation_distribution[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Analysis, Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_spatial_probabilities(hmm, matrix, title):\n",
    "    size = int(len(hmm.state_space)**0.5)\n",
    "    assert len(hmm.state_space) == size*size\n",
    "    \n",
    "    f, axs = plt.subplots(size, size,  constrained_layout=True)\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            state1 = (x,y)\n",
    "            ax = axs[y][x]\n",
    "            ax.set_title(f\"Z_t=({x},{y})\")\n",
    "            data=np.zeros((size, size))\n",
    "            for x2 in range(size):\n",
    "                for y2 in range(size):\n",
    "                    state2 = (x2,y2)\n",
    "                    data[y2,x2] = matrix[state1][state2]\n",
    "\n",
    "            im = ax.imshow(data)\n",
    "            ax.axes.xaxis.set_visible(False)\n",
    "            ax.axes.yaxis.set_visible(False)\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    f.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_errors(observations, ground_truth, hmm):\n",
    "    errors={}\n",
    "    for inference_method in [\"forward\", \"forward_backward\"]:\n",
    "        \n",
    "        inferred_trajectory = []\n",
    "        for t in range(1, 1+len(observations)):\n",
    "            if inference_method == \"forward\":\n",
    "                distribution = hmm.forward(observations[:t])\n",
    "            elif inference_method == \"forward_backward\":\n",
    "                distribution = hmm.forward_backward(observations, t)\n",
    "\n",
    "            \n",
    "            expected_x = expectation(distribution, lambda xy: xy[0])\n",
    "            expected_y = expectation(distribution, lambda xy: xy[1])\n",
    "            inferred_trajectory.append((expected_x, expected_y))\n",
    "        \n",
    "        errors[inference_method] = sum( (x-xh)**2 + (y-yh)**2\n",
    "                      for (x,y), (xh,yh) in zip(ground_truth, inferred_trajectory) )\n",
    "\n",
    "    return errors\n",
    "        \n",
    "def visualize_inferred_trajectory(observations, ground_truth, hmm):\n",
    "    size = int(len(hmm.state_space)**0.5)\n",
    "    assert len(hmm.state_space) == size*size\n",
    "\n",
    "    colors = \"rgbym\"\n",
    "    def plot_trajectory(ax, trajectory, label):\n",
    "        nonlocal colors\n",
    "        \n",
    "        n_segments = len(trajectory) - 1\n",
    "        for i in range(n_segments):\n",
    "            ax.plot(np.array([x for x,y in trajectory[i:i+2] ]),\n",
    "                     np.array([y for x,y in trajectory[i:i+2] ]),\n",
    "                     \"o-\",\n",
    "                     color=colors[0],\n",
    "                     alpha=(i/n_segments)*0.85+0.15,\n",
    "                     label=label if i==n_segments-1 else None)\n",
    "\n",
    "        colors = colors[1:]\n",
    "\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2,\n",
    "                                                 sharex=True, sharey=True)\n",
    "    \n",
    "    plot_trajectory(ax1, observations, \"observed data\")\n",
    "    plot_trajectory(ax2, ground_truth, \"true data\")        \n",
    "\n",
    "    errors={\"forward\":0.,\n",
    "            \"forward_backward\":0.}\n",
    "    \n",
    "    for inference_method, ax in zip([\"forward\", \"forward_backward\"], [ax3, ax4]):\n",
    "        \n",
    "        inferred_trajectory = []\n",
    "        for t in range(1, 1+len(observations)):\n",
    "            if inference_method == \"forward\":\n",
    "                distribution = hmm.forward(observations[:t])\n",
    "            elif inference_method == \"forward_backward\":\n",
    "                distribution = hmm.forward_backward(observations, t)\n",
    "\n",
    "            \n",
    "            expected_x = expectation(distribution, lambda xy: xy[0])\n",
    "            expected_y = expectation(distribution, lambda xy: xy[1])\n",
    "            inferred_trajectory.append((expected_x, expected_y))\n",
    "        \n",
    "        errors[inference_method] = sum( (x-xh)**2 + (y-yh)**2\n",
    "                      for (x,y), (xh,yh) in zip(ground_truth, inferred_trajectory) )\n",
    "        print(inference_method, \"has sum of squared error\", errors[inference_method])\n",
    "            \n",
    "\n",
    "        plot_trajectory(ax, inferred_trajectory, inference_method)\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax3.legend()\n",
    "    ax4.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a tracker model's probability distributions (model 1)\n",
    "The following code box will visualize the transition distribution $P(Z_{t+1}|Z_t)$ (top) followed by visualization of $P(X_t|Z_t)$ (bottom) for $\\alpha=5$, $\\beta=0.1$, $\\epsilon=0.001$.\n",
    "\n",
    "You should see a grid of heatmaps. Each heatmap is labelled with the value of $Z_t$. Each cell in the heatmap corresponds to a different value of either $Z_{t+1}$ (for the transition distribution, top grid) or $X_t$ (for the observation distribution, bottom grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 26 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Tracker(size=5, # size of the grid world\n",
    "                alpha=5, # how peaky the observation distribution is\n",
    "                beta=0.1, # how peaky the transition distribution is\n",
    "                epsilon=0.001 # probability of garbage sensor reading\n",
    ")\n",
    "visualize_spatial_probabilities(model, model.transition_distribution, \"model 1, log P(Z_{t+1}|Z_t)\")\n",
    "visualize_spatial_probabilities(model, model.observation_distribution, \"model 1, log P(X_t|Z_t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing a tracker model's probability distributions (model 2)\n",
    "The following code box will visualize the transition distribution $P(Z_{t+1}|Z_t)$ (top) followed by visualization of $P(X_t|Z_t)$ (bottom) for $\\alpha=0.1$, $\\beta=5$, $\\epsilon=0.5$.\n",
    "\n",
    "You should see a grid of heatmaps. Each heatmap is labelled with the value of $Z_t$. Each cell in the heatmap corresponds to a different value of either $Z_{t+1}$ (for the transition distribution, top grid) or $X_t$ (for the observation distribution, bottom grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tracker(size=5, # size of the grid world\n",
    "                alpha=0.1, # how peaky the observation distribution is\n",
    "                beta=5, # how peaky the transition distribution is\n",
    "                epsilon=0.5 # probability of garbage sensor reading\n",
    ")\n",
    "visualize_spatial_probabilities(model, model.transition_distribution, \"model 2, log P(Z_{t+1}|Z_t)\")\n",
    "visualize_spatial_probabilities(model, model.observation_distribution, \"model 2, log P(X_t|Z_t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing visualizations\n",
    "\n",
    "## Question 4.1 (2 pt)\n",
    "Which parameter(s) cause the observation distributions to differ? Why is model 1's observation distribution more peaky? Pay attention to the scale of the heatmaps in the legends, and remember that these are log probabilities. \n",
    "\n",
    "* **Solution: Alpha causes the observation distributions to differ. Alpha controls how likely it is for the sensor to report a location close to the true location. As a result, with a bigger alpha, it is more likely for the observed data to be near the true location, thus model 1's observation distribution is more peaky.** \n",
    "\n",
    "## Question 4.2 (2 pt)\n",
    "Which parameter(s) cause the transition distributions to differ? Which tracker model (1 or 2) has the more diffuse transition probabilities? \n",
    "\n",
    "* **Solution: Beta causes the transition distribution to differ. With higher beta, the next state location will be closer to the locaiton at previous state. As a result, for model 2, probability of next state locations being farther away from the previous location becomes really low, which explains the scale of log probability being -150 whlie model 1's scale is around -5 (the bigger the scale, the smaller the probability).** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an example trajectory on a 10x10 grid\n",
    "The following code creates an example trajectory that first walks upward along the side of the grid world, and then walks rightward along the top of the grid world. The moving object starts at (0,0). The sensor readings are correct except for the first sensor reading, which erroneously reports the garbage value of (4,7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10\n",
    "ground_truth = [(0,t) for t in range(0, size-1) ] + [(t,size-1) for t in range(size) ]\n",
    "observations = list(ground_truth)\n",
    "observations[0] = (4,7) # add a garbage sensor reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the inferred trajectory, using both forward and forward/backward, assuming sensor readings are sometimes garbage (epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_inferred_trajectory(observations, ground_truth,\n",
    "                              Tracker(size=10, alpha=1, beta=1, epsilon=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.3 (2 pt)\n",
    "Why is the forward estimate of the trajectory so incorrect at the beginning, and why does that inaccuracy not occur for forward/backward? \n",
    "\n",
    "* **Solution: Because observed data is very different from the true data. Forward estimate takes into account this difference which makes the estimate deviate a lot from true data. However, forward/backward looks at backward estimate, discards the incorrect forward estimate.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the inferred trajectory, using both forward and forward/backward, assuming sensor readings are never garbage (epsilon=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_inferred_trajectory(observations, ground_truth,\n",
    "                              Tracker(size=10, alpha=1, beta=1, epsilon=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.4 (2 pt)\n",
    "Why is the forward/backward estimate of the trajectory so incorrect at the beginning ($t=1$) when $\\epsilon=0$, compared to $\\epsilon=0.01$? Why is it still so incorrect at $t=2$?\n",
    "\n",
    "* **Solution: Since sensor readings are never garbage, meaning the deviated data is also correct. As a reuslt, forward/backward estimate cannot discard the initial jummps, leading to larger error.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the average error using both the forward and forward/backward algorithm\n",
    "The below code simulates 100 random trajectories, uses your inference methods to predict the trajectory, and builds analyses the sum of squared errors between the true trajectory and  the predicted trajectory at each time point. We are interested comparing the forward and forward/backward algorithm, so we build a histogram of the difference between the forward/backward error and the forward error (negative numbers mean that forward/backward is better).\n",
    "The vertical line shows the average of this difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tracker(size=5, # size of the grid world\n",
    "                alpha=2, # how peaky the observation distribution is\n",
    "                beta=2, # how peaky the transition distribution is\n",
    "                epsilon=0.01 # probability of garbage sensor reading\n",
    ")\n",
    "\n",
    "    \n",
    "difference_in_errors = []\n",
    "for s in range(100):\n",
    "    np.random.seed(s)\n",
    "    ground_truth, observations=model.sample_from_model(20)\n",
    "    errors = analyze_errors(observations, ground_truth, model)\n",
    "\n",
    "    difference_in_errors.append(errors[\"forward_backward\"]-errors[\"forward\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(difference_in_errors)\n",
    "plt.xlabel(\"forward/backward error, minus forward error\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.axvline(np.array(difference_in_errors).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.5 (4 pt)\n",
    "You should see that on average forward/backward is more accurate. What is a real world example of tracking a moving object where you would use forward/backward? What is a real world example of tracking a moving object where you would use forward instead? \n",
    "\n",
    "* **Solution: When all data is given, we can use forward/backward algorithm to smooth out the observations. However, when data is given in real time, we can only use forward algorithm.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark the scaling behavior of your inference algorithms\n",
    "The below code simulates 100 random trajectories of length between 1..100, and benchmarks the runtime of your forward, backward, and forward/backward inference methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_inference(hmm):\n",
    "\n",
    "    forward_time, backward_time, forward_backward_time = [], [], []\n",
    "    \n",
    "    for t in range(1, 100+1):\n",
    "        ground_truth, observations = hmm.sample_from_model(t)\n",
    "        hmm.forward_dynamic_programming = {}\n",
    "        hmm.backward_dynamic_programming = {}\n",
    "\n",
    "        start_time = time()\n",
    "        hmm.forward(observations)\n",
    "        total_time = time() - start_time\n",
    "        \n",
    "        forward_time.append(total_time)\n",
    "\n",
    "        hmm.forward_dynamic_programming = {}\n",
    "        hmm.backward_dynamic_programming = {}\n",
    "        start_time = time()\n",
    "        hmm.backward(observations)\n",
    "        total_time = time() - start_time\n",
    "        \n",
    "        backward_time.append(total_time)\n",
    "\n",
    "        hmm.forward_dynamic_programming = {}\n",
    "        hmm.backward_dynamic_programming = {}\n",
    "        start_time = time()\n",
    "        hmm.forward_backward(observations, max(1, t//2))\n",
    "        total_time = time() - start_time\n",
    "        \n",
    "        forward_backward_time.append(total_time)\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(len(forward_time))+1,\n",
    "             np.array(forward_time),\n",
    "             label=\"forward\")\n",
    "    plt.plot(np.arange(len(backward_time))+1,\n",
    "             np.array(backward_time),\n",
    "             label=\"backward\")\n",
    "    plt.plot(np.arange(len(forward_backward_time))+1,\n",
    "             np.array(forward_backward_time),\n",
    "             label=\"forward_backward\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"# observations\")\n",
    "    plt.ylabel(\"time (seconds)\")\n",
    "    plt.show()\n",
    "benchmark_inference(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.6 (1 pt)\n",
    "What is the big-O time complexity of these methods, in terms of the length of the observation sequence, $T$? \n",
    "\n",
    "* **Solution: O(T)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heartrate wakefulness model\n",
    "\n",
    "You will be working with a HMM for monitoring heart rate (the observed data) and inferring if the patient is awake or asleep (the hidden state). Time is measured in hours, and is discrete. **Heart rate is a continuous random variable.**\n",
    "\n",
    "The hidden state at each time point is whether the patient is awake or asleep:\n",
    "$$Z_t\\in \\{\\text{awake}, \\text{sleeping}\\}$$\n",
    "\n",
    "The observation at each time point is the heartrate:\n",
    "$$X_t\\in \\mathbb{R}$$\n",
    "\n",
    "Heart rate is distributed according to a normal (Gaussian) distribution, but the mean and standard deviation of this normal distribution depends on the hidden state according to the parameters $\\mu_\\text{sleep}, \\sigma_\\text{sleep}, \\mu_\\text{awake}, \\sigma_\\text{awake}$:\n",
    "$$p(X_t|Z_t=\\text{sleeping})=\\frac{1}{\\sigma_\\text{sleep}\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left( \\frac{X_t-\\mu_\\text{sleep}}{\\sigma_\\text{sleep}} \\right)^2  \\right)$$\n",
    "$$p(X_t|Z_t=\\text{awake})=\\frac{1}{\\sigma_\\text{awake}\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left( \\frac{X_t-\\mu_\\text{awake}}{\\sigma_\\text{awake}} \\right)^2  \\right)$$\n",
    "\n",
    "With probability $p_\\text{awaken}$, the patient wakes up after each hour spent sleeping:\n",
    "$$P(Z_t=\\text{awake}|Z_{t-1}=\\text{sleeping})=p_\\text{awaken}$$\n",
    "$$P(Z_t=\\text{sleeping}|Z_{t-1}=\\text{sleeping})=1-p_\\text{awaken}$$\n",
    "\n",
    "With probability $p_\\text{sleep}$, the patient falls asleep after each hour spent awake:\n",
    "$$P(Z_t=\\text{sleeping}|Z_{t-1}=\\text{awake})=p_\\text{sleep}$$\n",
    "$$P(Z_t=\\text{awake}|Z_{t-1}=\\text{awake})=1-p_\\text{sleep}$$\n",
    "\n",
    "Apriori, the patient is equally likely to be awake or asleep at the initial time step:\n",
    "$$P(Z_1=\\text{sleeping})=\\frac{1}{2}$$\n",
    "$$P(Z_1=\\text{awake})=\\frac{1}{2}$$\n",
    "\n",
    "## Task 5.1 Implement the constructor (2 pt)\n",
    "Implement the constructor of `SleepHeartRate`. You should calculate and set `transition_distribution` and `initial_distribution`.\n",
    "\n",
    "## Task 5.2 Implement `logProb_observation_given_state` (2 pt)\n",
    "Implement the method `logProb_observation_given_state`. Should return the logarithm of the probability density $p(X_t|Z_t)$.\n",
    "\n",
    "## Task 5.3 Implement `sample_observation_given_state` (2 pt)\n",
    "Implement the method `sample_observation_given_state` using `np.random.normal`, whose documentation is [here](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "from math import sqrt\n",
    "\n",
    "class SleepHeartRate(HMM):\n",
    "    def __init__(self, mu_sleep, mu_awake, sigma_sleep, sigma_awake,\n",
    "                 p_awaken, p_sleep):\n",
    "        \"\"\"\n",
    "        mu_sleep: average heart rate, sleeping\n",
    "        mu_awake: average heart rate, awake\n",
    "        sigma_sleep: standard deviation of heart rate, sleeping\n",
    "        sigma_awake: standard deviation of heart rate, awake\n",
    "        p_awaken: probability of waking up when asleep (per hour)\n",
    "        p_sleep: probability of falling asleep when awake (per hour)\n",
    "        \"\"\"\n",
    "\n",
    "        self.mu_sleep, self.mu_awake, self.sigma_sleep, self.sigma_awake = \\\n",
    "                       mu_sleep, mu_awake, sigma_sleep, sigma_awake\n",
    "\n",
    "        state_space = [\"sleeping\", \"awake\"]\n",
    "\n",
    "        # Populate `transition_distribution` so that transition_distribution[s1][s2]=log P(Z_{t+1}=s2|Z_t=s1)\n",
    "        # s1, s2 should be either \"sleeping\" or \"awake\"\n",
    "        transition_distribution = {}\n",
    "        \n",
    "        transition_distribution[\"sleeping\"] = {}\n",
    "        transition_distribution[\"awake\"] = {}\n",
    "        transition_distribution[\"sleeping\"][\"awake\"] = log(p_awaken)\n",
    "        transition_distribution[\"sleeping\"][\"sleeping\"] = log(1 - p_awaken)\n",
    "        transition_distribution[\"awake\"][\"sleeping\"] = log(p_sleep)\n",
    "        transition_distribution[\"awake\"][\"awake\"] = log(1 - p_sleep)\n",
    "        \n",
    "        # Populate `initial_state_distribution` such that initial_state_distribution[z]=log P(Z_1=z)\n",
    "        # z should be either \"sleeping\" or \"awake\"\n",
    "        initial_state_distribution = {}\n",
    "        \n",
    "        initial_state_distribution[\"sleeping\"] = log(1/2)\n",
    "        initial_state_distribution[\"awake\"] = log(1/2)\n",
    "        \n",
    "        super().__init__(state_space, transition_distribution, initial_state_distribution)\n",
    "\n",
    "    def logProb_observation_given_state(self, observation, state):\n",
    "        \"\"\"\n",
    "        calculates log p(observation_t | state_t)\n",
    "        \"\"\"\n",
    "        if (state == \"sleeping\"):\n",
    "            return log(exp((-1/2)*(((observation - self.mu_sleep) / self.sigma_sleep)**2))/(self.sigma_sleep*sqrt(2*pi)))\n",
    "        else:\n",
    "            return log(exp((-1/2)*(((observation - self.mu_awake) / self.sigma_awake)**2))/(self.sigma_awake*sqrt(2*pi)))\n",
    "\n",
    "    def sample_observation_given_state(self, state):\n",
    "        \"\"\"\n",
    "        stochastically draws a sample from P(observation_t | state_t)\n",
    "        \"\"\"\n",
    "        if state == \"sleeping\":\n",
    "            return np.random.normal(self.mu_sleep, self.sigma_sleep)\n",
    "        else:\n",
    "            return np.random.normal(self.mu_awake, self.sigma_awake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis code for heart rate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_predictions(ground_truth, observations, model,\n",
    "                              visualize=True):\n",
    "    T = len(observations)\n",
    "    assert T == len(ground_truth)\n",
    "\n",
    "    ground_truth = [(gt==\"awake\")*1. for gt in ground_truth ]\n",
    "\n",
    "    if visualize:\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax1.set_xlabel('time (hr)')\n",
    "        ax1.set_ylabel('beats per minute', color=\"b\")\n",
    "        ax1.plot(observations, label=\"heart rate\", color=\"b\")\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        ax2.set_ylabel('Probability Awake?')  # we already handled the x-label with ax1\n",
    "        ax2.plot(ground_truth, color=\"r\", label=\"ground truth\")\n",
    "\n",
    "    errors = {}\n",
    "\n",
    "    for inference_method in [\"forward\", \"forward_backward\"]:\n",
    "        \n",
    "        inferred_trajectory = []\n",
    "        for t in range(1, 1+len(observations)):\n",
    "            if inference_method == \"forward\":\n",
    "                distribution = model.forward(observations[:t])\n",
    "            elif inference_method == \"forward_backward\":\n",
    "                distribution = model.forward_backward(observations, t)\n",
    "            \n",
    "            expected_awake = expectation(distribution, lambda state: 1.*(state==\"awake\"))\n",
    "            inferred_trajectory.append(expected_awake)\n",
    "        \n",
    "        errors[inference_method] = sum( (a-ah)**2 \n",
    "                                        for a, ah in zip(ground_truth, inferred_trajectory) )\n",
    "        if visualize:\n",
    "            print(inference_method, \"has sum of squared error\", errors[inference_method])\n",
    "            ax2.plot(inferred_trajectory, label=inference_method)\n",
    "            \n",
    "\n",
    "    \n",
    "    if visualize:\n",
    "        ax2.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a heart rate model, simulate random data, and run both forward and forward/backward inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SleepHeartRate(50., # average heart rate, sleeping\n",
    "                       85., # average heart rate, awake\n",
    "                       5.,  # standard deviation of heart rate, sleeping\n",
    "                       15., # standard deviation of heart rate, awake\n",
    "                       1/8., # probability of waking up when asleep (per hour)\n",
    "                       1/16. # probability of falling asleep when awake (per hour)\n",
    ")\n",
    "np.random.seed(0)\n",
    "ground_truth, observations=model.sample_from_model(48)\n",
    "analyze_model_predictions(ground_truth, observations, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.1 (2 pt)\n",
    "You should notice at least one long run of the patient being awake. You should also notice that the forward/backward algorithm \"jumps the gun\" and starts predicting a small probability of wakefulness before actually the patient is awake. Why does this happen?\n",
    "\n",
    "* **Solution: Because forward/backward uses later data to smooth out the estimate, and since heart rate increases significantly right after, forward/backward takes into account this change and predicts that the patient is awake.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the average error using both the forward and forward/backward algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_in_errors = []\n",
    "for s in range(100):\n",
    "    np.random.seed(s)\n",
    "    ground_truth, observations=model.sample_from_model(20)\n",
    "    errors = analyze_model_predictions(ground_truth, observations, model, visualize=False)\n",
    "\n",
    "    difference_in_errors.append(errors[\"forward_backward\"]-errors[\"forward\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(difference_in_errors, bins=20)\n",
    "plt.xlabel(\"forward/backward error, minus forward error\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.axvline(np.array(difference_in_errors).mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6.2 (4 pt) \n",
    "You should see that on average forward/backward is more accurate. What is a real world example of monitoring heart rate and inferring wakefulness where you would use forward/backward? What is a real world example in this scenario where you would use forward instead? \n",
    "\n",
    "* **Solution: When you want more accurate data and has all the data given to you, you want to use forward/backward. When it is real-time data and you want to make sure that a patient is awake, then you would use forward.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A mystery heartrate data set for you to analyze\n",
    "We have generated a dataset -- but this dataset was generated from a patient with a unknown sleeping heartrate $\\mu_\\text{sleep}$, which you will have to infer. You can assume\n",
    "$\\sigma_\\text{sleep}=5, \\mu_\\text{awake}=85, \\sigma_\\text{awake}=15, p_\\text{awaken}=1/8, p_\\text{sleep}=1/16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [60.024586292581, 52.3718697759278, 58.80170020585246, 67.11909523460665, 49.8754588478012, 65.14986012024706, 64.24568967142685, 100.04802032130739, 99.8591886252985, 81.5722175046971, 98.15841662729339, 94.52084751694113, 69.18268915795875, 55.57529555049684, 58.5367056334972, 71.17802518532243, 99.45343945778617, 110.00871336445333, 65.87609719134765, 73.49688388469505, 75.14999745087842, 62.83556592039949, 62.24268945549414, 50.64006868902254, 58.68316622316542, 58.76844539881316, 44.70217969599618, 53.20784968688581, 59.6254161863151, 52.73053617276515, 91.46946843489923, 67.56155611791766, 96.00146670782686, 93.16447851190654, 80.33287865068364, 87.14571434632957, 68.70089221350426, 80.60219494419252, 65.94141431130146, 96.37328628188756, 55.11221509740634, 51.2383376346944, 56.65657395145553, 60.49329228703645, 50.386167344456574, 58.28615586781797, 59.037250815880824, 52.947108422631445, 60.45578535635233, 52.80251226623432, 46.75169525578589, 57.22571676296071, 57.52007078129511, 61.60973314211153, 57.34122092576476, 55.89793390034725, 52.01143792919454, 54.980726069973045, 65.70825104401858, 63.02214790547422, 52.383937679940246, 68.46791656760132, 50.5208261485005, 56.596465482996926, 50.46997663314116, 52.18782319819992, 47.75103007441167, 52.16949896921022, 54.72292078866861, 60.91076777688642, 66.73557016014277, 51.788437149295, 47.9436462280157, 58.17907883132434, 58.116637693881735, 74.47731618079851, 72.00486966527343, 71.44157933564942, 51.351651546480895, 48.59728122511345, 48.51399586787, 59.98179888718437, 52.5981219814829, 51.405716498380116, 55.039190955042805, 61.0988621629985, 82.64547636716155, 88.78430366800575, 82.37630040728104, 71.97660245392298, 80.54323815406585, 109.64690694799344, 105.35659649433344, 104.9638969131752, 93.7568541513724, 92.7414755052842, 70.81475837717642, 99.9090452664229, 95.25439425732922, 84.1307548628451, 104.82574081316936, 81.96191274406061, 70.86757270765774, 74.41525964799092, 85.79120242387785, 86.1332117596878, 100.7678626743189, 90.01624012160912, 71.7884040795559, 70.73359696143602, 101.20891696064376, 67.29279483976377, 93.96488663365152, 89.00692866991777, 86.50565099080158, 94.16774529814853, 68.46851293124942, 50.31707874861571, 95.48121164553594, 93.3282366893721, 107.58874005602294, 74.26097918643995, 108.58480530604129, 65.30861662737179, 68.2582493593002, 105.57462867649608, 89.34729173581606, 90.17579760799975, 63.754670231991454, 92.93975937886121, 60.30811678140145, 52.018810065895956, 59.65207940890939, 52.44195306378311, 50.44592662162375, 60.77384356589032, 95.00224569796306, 63.05121895867677, 56.33014259948891, 64.57639305225467, 96.9959748510934, 83.59908000182357, 77.69382924661042, 91.86627110283584, 90.4049936295561, 99.47473788932902, 66.10634898740406, 118.80472907658842, 100.75677458241091, 88.2343328762152, 71.874381123758, 108.6305918624782, 56.70109221698572, 55.44656624810501, 56.864066289101864, 52.4095087326326, 58.11733066507287, 60.38281252479621, 61.122670007845926, 81.03674262733746, 100.2212207602823, 84.09680007042657, 51.25870284998918, 46.22507411109206, 56.931820829501596, 48.204974739723156, 48.86145443933335, 102.57440152785577, 54.05309659353633, 53.34568876774659, 80.7273980640838, 95.59286161718398, 77.88622665296381, 82.40312847033421, 108.00156058411162, 81.90258450297034, 74.1424824500362, 78.42103682348184, 79.49607574109665, 87.94096191621301, 84.05112114463488, 70.03983307606197, 96.88259300600808, 89.19818739612077, 90.34309722760493, 55.85955559057182, 55.561348883547666, 56.21942910813162, 62.65734767196688, 91.81887482725234, 87.14927267146705, 79.60907688266254, 92.8416840816192, 77.54922167157379, 92.38062636596855, 108.69096015624399, 62.46034405153159, 76.87859491768087, 84.1845484157788, 90.13390354517759, 62.41964107344813, 87.89317766344834, 53.95283286979039, 85.74609210621546, 78.8456726565378, 110.96926715915241, 78.68033912662743, 97.30225960808389, 110.74538847351698, 77.71722971641593, 93.01482784188605, 118.1580589938144, 61.16943822368874, 92.40224715512375, 113.88047023655719, 77.42430300423602, 91.3856464843722, 112.1463012616061, 69.20660687884333, 75.2474826323996, 51.67908547001017, 49.496102817092705, 58.35683262150556, 56.597154491810606, 62.576894102206296, 44.382660204462, 55.12491945581128, 59.45083658054802, 45.460929928686205, 59.850429145895575, 52.301922385277955, 50.56818128833715, 54.03572641501221, 53.91969524630194, 51.700234165529395, 55.41156912571518, 63.475033661833294, 58.64556459921754, 51.88113967789142, 47.201143524742875]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Calculate the prior, likelihood, and posterior over the sleeping heart rate, given the mystery dataset\n",
    "\n",
    "Assume that we think that the sleeping heart rate is between $40$ and $65$ beats per minute, but that it is probably around $45$.\n",
    "\n",
    "We are going to quantize the sleeping heart rate:\n",
    "$\\mu_\\text{sleep}\\in \\left\\{ 40, 41, 42, \\cdots, 65 \\right\\}$. In the code below, `possible_sleeping_heart_rates` contains the list of possible sleeping heart rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_sleeping_heart_rates = list(range(40, 65+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.1 Code the prior over $\\mu_\\text{sleep}$ (1pt)\n",
    "Assume this prior distribution over $\\mu_\\text{sleep}$:\n",
    "$$P(\\mu_\\text{sleep})\\propto 1\\left[ \\mu_\\text{sleep}\\in \\left\\{ 40, 41, 42, \\cdots, 65 \\right\\} \\right]\\times\\exp\\left( -|\\mu_\\text{sleep}-45| \\right)$$\n",
    "Calculate `log_prior`, which should be a dictionary mapping $\\mu_\\text{sleep}$ to $\\log P(\\mu_\\text{sleep})$. It should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_prior[hr] = log P(mu_sleep=hr), whenever hr is a member of possible_sleeping_heart_rates\n",
    "log_prior = {}\n",
    "\n",
    "for i in possible_sleeping_heart_rates:\n",
    "    log_prior[i] = log(exp(-abs(i - 45)))\n",
    "    \n",
    "log_prior = norm_log_distribution(log_prior)\n",
    "\n",
    "# Check that it is normalized\n",
    "normalizing_constant = log_sum_exp_list(list(log_prior.values()))\n",
    "if abs(normalizing_constant)>1e-5:\n",
    "    assert False, f\"Your prior over sleeping heart rates is not normalized. Remembered that it should contain log probabilities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.2 Code the likelihood of the mystery data given $\\mu_\\text{sleep}$ (5pt)\n",
    "Assume this likelihood:\n",
    "$$P(X_{1:T}|\\mu_\\text{sleep})=\\sum_{Z_{1:T}}P(X_{1:T}, Z_{1:T}|\\mu_\\text{sleep}, \\sigma_\\text{sleep}=5, \\mu_\\text{awake}=85, \\sigma_\\text{awake}=15, p_\\text{awaken}=1/8, p_\\text{sleep}=1/16)\n",
    "$$\n",
    "where $X_{1:T}$ is the mystery data, stored in the variable `data`.\n",
    "\n",
    "Calculate `log_likelihood`, which should be a dictionary mapping $\\mu_\\text{sleep}$ to $\\log P(X_{1:T}|\\mu_\\text{sleep})$.\n",
    "You should use the `.marginal_likelihood` method that you defined in the `HMM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_likelihood[hr] = log P(data|mu_sleep=hr), whenever hr is a member of possible_sleeping_heart_rates\n",
    "log_likelihood = {} # implement as part of homework\n",
    "\n",
    "for i in possible_sleeping_heart_rates:\n",
    "    model = SleepHeartRate(i, 85., 5., 15., 1/8., 1/16.)\n",
    "    log_likelihood[i] = model.marginal_likelihood(data)\n",
    "    \n",
    "log_likelihood = norm_log_distribution(log_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7.3 Code the posterior over $\\mu_\\text{sleep}$ given the mystery data (2pt)\n",
    "By Bayes theorem, the posterior $P(\\mu_\\text{sleep}|X_{1:T})$ is\n",
    "$$\n",
    "P(\\mu_\\text{sleep}|X_{1:T})\\propto P(X_{1:T}|\\mu_\\text{sleep})P(\\mu_\\text{sleep})\n",
    "$$\n",
    "where $X_{1:T}$ is the mystery data.\n",
    "\n",
    "Calculate `log_posterior`, which should be a dictionary mapping $\\mu_\\text{sleep}$ to $\\log P(\\mu_\\text{sleep}|X_{1:T})$. It should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_posterior[hr] = log P(mu_sleep=hr|data), whenever hr is a member of possible_sleeping_heart_rates\n",
    "log_posterior = {} # implement as part of homework\n",
    "for i in possible_sleeping_heart_rates:\n",
    "    log_posterior[i] = log_sum_exp(log_prior[i], log_likelihood[i])\n",
    "log_posterior = norm_log_distribution(log_posterior)\n",
    "\n",
    "# Checked that it is normalized\n",
    "normalizing_constant = log_sum_exp_list(list(log_posterior.values()))\n",
    "if abs(normalizing_constant)>1e-5:\n",
    "    assert False, f\"Your posterior over sleeping heart rates is not normalized. Remembered that it should contain log probabilities.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the prior, likelihood, and posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, sharex=True, sharey=False)\n",
    "ax1.plot(possible_sleeping_heart_rates,\n",
    "         [exp(log_prior[hr]) for hr in possible_sleeping_heart_rates ],\n",
    "         label=\"prior\")\n",
    "ax1.set_xlabel(\"sleeping heart rate\")\n",
    "ax1.set_ylabel(\"prior: P(sleeping heart rate)\")\n",
    "ax1.legend()\n",
    "ax2.plot(possible_sleeping_heart_rates,\n",
    "         [log_likelihood[hr] for hr in possible_sleeping_heart_rates ],\n",
    "         label=\"log likelihood\")\n",
    "ax2.set_xlabel(\"sleeping heart rate\")\n",
    "ax2.set_ylabel(\"log likelihood: log P(data | sleeping heart rate)\")\n",
    "ax2.legend()\n",
    "ax3.plot(possible_sleeping_heart_rates,\n",
    "         [exp(log_posterior[hr]) for hr in possible_sleeping_heart_rates ],\n",
    "         label=\"posterior\")\n",
    "ax3.legend()\n",
    "ax3.set_xlabel(\"sleeping heart rate\")\n",
    "ax3.set_ylabel(\"posterior: P(sleeping heart rate | data)\")\n",
    "plt.ylabel(\"probability\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.4 (2pt)\n",
    "What is the most likely value of sleeping heart rate, conditioned on the data? Write a very small amount of code below that will print it out, and compute it from `log_posterior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(str(max(log_posterior, key=log_posterior.get)) + \" beats per minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.5 (2pt) \n",
    "What is the value of sleeping heart rate that assigns the highest probability to the data? Write a very small amount of code below that will print it out, and compute it from `log_likelihood`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(max(log_likelihood, key=log_likelihood.get)) + \" beats per minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7.6 (1pt) \n",
    "Why are these previous two values the same, or at least very close? Appeal to general principles of Bayesian inference\n",
    "\n",
    "* **Solution: They are far, but we can see from the posterior plot that it is clearly affected by the likelihood function, as probability of mu_sleep being 45 went down and there is a peak at around 60. In Bayesian inference, posterior is influenced by both prior and likelihood and as more data is measured, the more likely the posterior follows likelihood distribution. Here, there wasn't enough data collected to push the sleeping heart rate value away from prior belief.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
