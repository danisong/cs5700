{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5700 Homework\n",
    "\n",
    "Due: Wednesday May 4, 2022, 11:59pm\n",
    "\n",
    "Late deadline (50\\% off): Friday May 6, 2022, 11:59pm\n",
    "\n",
    "Name: Your Name\n",
    "\n",
    "NetID: Your NetID\n",
    "\n",
    "\n",
    "## Instructions\n",
    "Read through this python notebook. You will be asked questions and given programming tasks. These are numbered, followed by point values for grading (for example, \"Question 17.8 (5pt)\"). You should edit the notebook directly to input your solution code and question answers. You can double click on text to edit it, which you should use for putting your question answers in the notebook.\n",
    "\n",
    "**VERY IMPORTANT!** Your submission *needs* to contain the output results of running your notebook. We cannot guarantee that we will run your notebook if you do not provide the outputs, because running this notebook can take a while. It is possible that you will receive **zero points** if you do not provide the outputs of running the notebook. You can avoid this by carefully following the following submission instructions, and making sure that what you submit contains the outputs of your code.\n",
    "\n",
    "When you are ready to submit, you should run all of your code (click Cell->Run All). Then you should submit **three** files to gradescope:\n",
    "* PDF: export to PDF (click File->Download as->PDF via LaTeX)\n",
    "* .py: export to Python (click File->Download as->Python (.py))\n",
    "* .ipynb: submit the edited Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Base Class\n",
    "You will implement an MDP base class. The introduction of the class is in the comments below.\n",
    "\n",
    "## Task 1.1: Implement the ``sim_transition`` function (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, random, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# random sample from a (state -> probability) dict\n",
    "def sample(d):\n",
    "    states = [x[0] for x in d.items()]\n",
    "    probs  = [x[1] for x in d.items()]\n",
    "    return states[np.random.choice(len(states), p=probs)]\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    # Contains the following attributes:\n",
    "    # states: container for all the possible states, e.g., a list or a set\n",
    "    # actions: container for all the possible actions, e.g., a list or a set\n",
    "    # discount_factor: a real number, greater than 0, less than or equal to 1\n",
    "    # init_state: a distribution over all the states\n",
    "    # Contains 4 member functions:\n",
    "    # reward_fn: (state, action, state) -> r(state, action, state)\n",
    "    # terminal: state -> bool to check whether current MDP is stopped\n",
    "    # transition_model: (state, action) -> probability distribution over next state\n",
    "    # sim_transition: (state, action) -> (r(state, action), next_state) \n",
    "    #                 since many reward function has nothing to do with next state\n",
    "    #                 or \n",
    "    #                 (state, action, next_state) -> (r(state, action), next_state)\n",
    "    #                 Note, if state is a terminal state, please sample from the intial states\n",
    "\n",
    "    def __init__(self, states, actions, discount_factor, init_state):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.discount_factor = discount_factor\n",
    "        self.init_state = init_state\n",
    "\n",
    "    def terminal(self, s):\n",
    "        assert False, \"implement for a specific MDP in the child class\"\n",
    "\n",
    "    def reward_fn(self, s, a, sp):\n",
    "        assert False, \"implement for a specific MDP in the child class\"\n",
    "\n",
    "    def transition_model(self, s, a):\n",
    "        assert False, \"implement for a specific MDP in the child class\"\n",
    "    \n",
    "    # You can use anything from this class to implement this.\n",
    "    def sim_transition(self, s, a, sp=None):\n",
    "        # TODO: ASSIGNMENT\n",
    "        assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple MDP instance and Policy Evaluation\n",
    "\n",
    "This part is to help you be comfortable with working with a class inherited from the MDP base class\n",
    "\n",
    "## Task 1.2 (5 pts)\n",
    "\n",
    "You will be implmenting a simple infinite MDP class that consists of:\n",
    "1. Four states: 0, 1, 2, 3\n",
    "2. Two actions: 0, 1\n",
    "\n",
    "More intructions can be found in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class example_MDP(MDP):\n",
    "    # Inherited from the MDP class\n",
    "    # the reward function is given by a dict: (s, a, s^\\prime) -> r\n",
    "    # the discount factor is also given as an argument, note here it is  infinite MDP, so its range is [0, 1)\n",
    "    # the initial state is always 0\n",
    "    # the state transition function p is also given by a dict: (s, a) -> a probability distribution over all the states\n",
    "    def __init__(self, reward_function, state_transition_function, discount_factor):\n",
    "        #TODO: Assignment\n",
    "\n",
    "        assert (discount_factor >= 0 and discount_factor < 1)\n",
    "        assert False, \"Part of the Assignment\"\n",
    "\n",
    "    def terminal(self, s):\n",
    "        #TODO: Assignment\n",
    "        assert False, \"Part of the Assignment\"\n",
    "\n",
    "    def reward_fn(self, s, a, sp):\n",
    "        #TODO: Assignment\n",
    "        assert False, \"Part of the Assignment\"\n",
    "\n",
    "    def transition_model(self, s, a):\n",
    "        #TODO: Assignment\n",
    "        assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a real MDP as shown in the figure below (please run the next cell to get the figure) to test your code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"./simple_mdp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only plot the arrows where their transition probabilities are greater than 0.\n",
    "\n",
    "## Task 2.1 (2 pts)\n",
    "Based on the figure above, create variables `reward_function` and `state_transiton_function`, which should be compatible with being passed as arguments to the constructor of `example_MDP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Assignment\n",
    "reward_function = None # fill this in as part of the assignment\n",
    "state_transiton_function = None # fill this in as part of the assignment\n",
    "assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Evaulate a policy given full knowledge of the MDP. (10 pts)\n",
    "We know S, A, $p(s^\\prime|s, a)$ and $R(s, a, s^\\prime)$, so you will implement a **policy_evaluation_exact** function that can evaulate a deterministic policy for this MDP. \n",
    "\n",
    "As metioned in the slides, you can use Gaussian Elimination to solve the equation system. However, we do not require you to implement the Gaussian Elimination by yourself, you can use **np.linalg.solve** or any method you are comfortable with.\n",
    "\n",
    "More instructions are in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_exact(pi, mdp):\n",
    "    # Given an MDP class, evaulate the deterministic policy pi.\n",
    "    # Note, pi is a dict: s -> a.\n",
    "    # The return is expected to be a dict: s->the expected culmulitive reward w.r.t. pi of state s\n",
    "    # TODO: Assignment\n",
    "    assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an instance of example_MDP and a policy pi below. Then we can test your policy_evaluation_exact code. The discount_factor is 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mdp = example_MDP(reward_function, state_transition_function, discount_factor=0.5)\n",
    "pi = {}\n",
    "pi[0] = 0\n",
    "pi[1] = 0\n",
    "pi[2] = 1\n",
    "pi[3] = 0\n",
    "st_time = time.time()\n",
    "V = policy_evaluation_exact(pi, example_mdp)\n",
    "print (\"policy evaluation exact time cost: \", time.time() - st_time)\n",
    "answer_V = {0: 2.2745098039215685, 1: 2.0784313725490198, 2: 3.019607843137255, 3: 2.0392156862745097}\n",
    "error_eps = 1e-6\n",
    "flag = 0\n",
    "for k, v in V.items():\n",
    "    if np.abs(v - answer_V[k]) > error_eps:\n",
    "        flag = 1\n",
    "        raise ValueError(\"Wrong expected cumulative reward at state %d\" % (k))\n",
    "if not flag:\n",
    "    print (\"You have passed the example test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 Iterative Policy Evaluation (10 pts)\n",
    "In addition to solving the value function exactly as we did above, we can also solve it iteratively. Iterative policy evaluation is more efficient in general.\n",
    "\n",
    "The basic idea of iterative policy evaluation is to keep updating the value function as following:\n",
    "$$V(s) = \\sum_{s^\\prime \\in S}p(s^\\prime|s, \\pi(s))(R(s, \\pi(s), s^\\prime) + \\gamma V(s^\\prime))$$\n",
    "until the difference between the old value of $V(s)$ and the new value of $V(s)$ is small enough (use 1e-6 in this assignment), i.e., converge to the true value.\n",
    "\n",
    "You will be implmenting the **policy_evaluation_iterative** function. Please print the number of iteration used until convergence in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation_iterative(pi, mdp, init_V=None):\n",
    "    # Given an MDP class, evaulate the deterministic policy pi.\n",
    "    # Note, pi is a dict: s -> a.\n",
    "    # init_V is the estimated value for each state, if it is None, set 0 for all states\n",
    "    # The return is expected to be a dict: s->the expected culmulitive reward w.r.t. pi of state s\n",
    "    # TODO: Assignment\n",
    "    assert False, \"Part of the Assignment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_mdp = example_MDP(reward_function, state_transition_function, discount_factor=0.5)\n",
    "pi = {}\n",
    "pi[0] = 0\n",
    "pi[1] = 0\n",
    "pi[2] = 1\n",
    "pi[3] = 0\n",
    "st_time = time.time()\n",
    "V = policy_evaluation_iterative(pi, example_mdp)\n",
    "print (\"policy evaluaiton iterative time cost: \", time.time() - st_time)\n",
    "answer_V = {0: 2.2745098039215685, 1: 2.0784313725490198, 2: 3.019607843137255, 3: 2.0392156862745097}\n",
    "error_eps = 1e-6\n",
    "flag = 0\n",
    "for k, v in V.items():\n",
    "    if np.abs(v - answer_V[k]) > error_eps:\n",
    "        flag = 1\n",
    "        raise ValueError(\"Wrong expected cumulative reward at state %d\" % (k))\n",
    "if not flag:\n",
    "    print (\"You have passed the example test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important thing in iterative policy evaluation is the intial value, which impacts the efficiency of the algorithm. Try doing both a good guess and a bad guess of the value function below and see how the number of iteration varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Assignment\n",
    "good_init_V = [\"YOUR GOOD GUESS\"]\n",
    "bad_init_V = [\"VERY BAD GUESS\"]\n",
    "print (\"Good init V\")\n",
    "V = policy_evaluation_iterative(pi, example_mdp, init_V=good_init_V)\n",
    "print (\"All zero init V\")\n",
    "V = policy_evaluation_iterative(pi, example_mdp, init_V=None)\n",
    "print (\"Bad init V\")\n",
    "V = policy_evaluation_iterative(pi, example_mdp, init_V=bad_init_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Optimal Policy (5 pts)\n",
    "We have implmented two types of policy evaluation above. Now we want to find the optimal policy if we know S, A, $p(s^\\prime|s, a)$ and $R(s, a, s^\\prime$).\n",
    "You will be implementing the policy iteration algorithm step by step:\n",
    "1. Policy Evaluation (we have done in the Task 2)\n",
    "2. Policy Improvement\n",
    "3. Combine them toghther\n",
    "\n",
    "## Task 3.1 Policy Improvment\n",
    "Given a value function $V$ and an MDP instance, you want to find the best policy w.r.t. $V$. You will be implementing the **policy_improvment** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improment(V, mdp):\n",
    "    # Given an MDP class and value function V, return the optimal deterministic policy pi.\n",
    "    # Note, pi is a dict: s -> a.\n",
    "    # The return is expected to be a dict: s->a\n",
    "    # TODO: Assignment\n",
    "    assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 Policy Iteration (5 pts)\n",
    "You can implement the **policy_iteration** now. We consider three types of the policy evaluation step:\n",
    "1. one_step: as described in the slides, we only do one step of the Bellman equation.\n",
    "2. exact: solve the linear system exactly\n",
    "3. iterative: iteratively solve the linear system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp, mode=\"one_step\"):\n",
    "    # Given an mdp, you need to find the optimal policy pi: s->a\n",
    "    # Note pi is a dict: s->a and it is supposed to be the return\n",
    "    # TODO: Assignment\n",
    "    assert False, \"Part of the Assignment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the previous defined example_MDP class to test your policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = [\"one_step\", \"exact\", \"iterative\"]\n",
    "for mode in modes:\n",
    "    optimal_pi = policy_iteration(example_MDP, mode=mode)\n",
    "    V = policy_evaluation_iterative(optimal_pi, example_mdp)\n",
    "    error_eps = 1e-6\n",
    "    flag = 0\n",
    "    print (\"Now testing the policy iteration mode: %s\" % (mode))\n",
    "    for k, v in V.items():\n",
    "        if np.abs(v - answer_V[k]) > error_eps:\n",
    "            flag = 1\n",
    "            raise ValueError(\"Wrong expected cumulative reward at state %d\" % (k))\n",
    "    if not flag:\n",
    "        print (\"You have passed the example test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to play a real-game *No_Exit*\n",
    "After solving a simple MDP instance, we now are trying to learn to play a game called *No_Exit*. There is a figure about this game below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block to get the figure.\n",
    "display.Image(\"./no_exit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purple square is an object that will move following some rules. Two yellow squares toghther forms a paddle that a player can only move it up, down or stay in the origin place to prevent the purple square getting out of the area from the right. You can regard these two yellow squares are sticked firmly, so you are not control these two yellow squares separately.\n",
    "\n",
    "The specific definition of the game can be found below. Read the comments carefully, it defines the states, actions, transition probabilitys and the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class No_Exit(MDP):\n",
    "    # Like breakout or pong, but one player, no walls to break out, no\n",
    "    # way to win. You can move paddle vertically up or down or stay\n",
    "    # +1: vertically up\n",
    "    # 0 : stay\n",
    "    # -1: vertically down\n",
    "    actions = (+1, 0, -1) \n",
    "    \n",
    "    def __init__(self, field_size, ball_speed = 1, random_start = True):\n",
    "        # image space is n by n\n",
    "        self.q = None\n",
    "        self.n = field_size\n",
    "        h = self.n * ball_speed \n",
    "        self.discount_factor = (h - 1.0) / h\n",
    "        self.ball_speed = ball_speed\n",
    "        # state space is: ball position and velocity, paddle position\n",
    "        # and velocity\n",
    "        # - ball position is n by n\n",
    "        # - ball velocity is one of (-1, -1), (-1, 1), (0, -1), (0, 1),\n",
    "        #                          (1, -1), (1, 1)\n",
    "        # - ball velocity can be defined as (row velocity, col velocity)\n",
    "        # - paddle position is n; this is location of bottom of paddle,\n",
    "        #    can stick \"up\" out of the screen\n",
    "        #   the length of the paddle is 2 as showed in the figure above\n",
    "        # - paddle velocity is one of 1, 0, -1\n",
    "        self.states = [((br, bc), (brv, bcv), pp, pv) for \\\n",
    "                         br in range(self.n) for \n",
    "                         bc in range(self.n) for\n",
    "                         brv in (-1, 0, 1) for\n",
    "                         bcv in (-1, 1) for \n",
    "                         pp in range(self.n) for \n",
    "                         pv in (-1, 0, 1)]\n",
    "        # the ball runs out of the field\n",
    "        self.states.append('over')\n",
    "        #self.start = dist.uniform_dist([((br, 0), (0, 1), 0, 0) \\\n",
    "        #                                for br in range(self.n)]) \\\n",
    "        #        if random_start else  \\\n",
    "        #        dist.delta_dist(((int(self.n/2), 0), (0, 1), 0, 0))\n",
    "        init_states = [((br, 0), (0, 1), 0, 0) for br in range(self.n)]\n",
    "        self.init_state = {x : 1.0 / float(len(init_states)) for x in init_states}\n",
    "\n",
    "    ax = None\n",
    "    ims = None\n",
    "    \n",
    "    IS_COLAB = False\n",
    "    grid = None\n",
    "\n",
    "    def draw_state(self, state = None, pause = False):\n",
    "        def _update(self, state, pause):\n",
    "            if self.ax is None or self.IS_COLAB:\n",
    "                plt.ion()\n",
    "                plt.figure(facecolor=\"white\")\n",
    "                self.ax = plt.subplot()\n",
    "\n",
    "            if state is None: state = self.state\n",
    "            ((br, bc), (brv, bcv), pp, pv) = state\n",
    "            im = np.zeros((self.n, self.n+1))\n",
    "            im[br, bc] = -1\n",
    "            im[pp, self.n] = 1\n",
    "            self.ax.cla()\n",
    "            self.ims = self.ax.imshow(im, interpolation = 'none',\n",
    "                                    cmap = 'viridis', \n",
    "                                    extent = [-0.5, self.n+0.5,\n",
    "                                                -0.5, self.n-0.5],\n",
    "                                    animated = True)\n",
    "            self.ims.set_clim(-1, 1)\n",
    "            plt.pause(0.0001) \n",
    "            if pause: input('go?')\n",
    "            else: plt.pause(0.1 if self.IS_COLAB else 0.01) \n",
    "\n",
    "        if self.IS_COLAB:\n",
    "            with self.grid.output_to(0, (self.parity % 10)):\n",
    "                # _update(self, state, pause)\n",
    "                self.grid.clear_cell(0, (self.parity + 1) % 10)\n",
    "                self.parity =  (self.parity + 9) % 10\n",
    "        else:\n",
    "            _update(self, state, pause)\n",
    "\n",
    "    def terminal(self, state):\n",
    "        return state == 'over'\n",
    "\n",
    "    def reward_fn(self, s, a, sp):\n",
    "        return 0 if s == 'over' else 1\n",
    "        \n",
    "    def transition_model(self, s, a, p=0.4):\n",
    "        # Only randomness is in brv and brc after a bounce\n",
    "        # 1- prob of negating nominal velocity\n",
    "        ret_distribution = {x : 0 for x in self.states}\n",
    "        if s == 'over':\n",
    "            ret_distribution[s] = 1.0\n",
    "            return ret_distribution\n",
    "        # Current state\n",
    "        ((br, bc), (brv, bcv), pp, pv) = s\n",
    "        # Nominal next ball state\n",
    "        new_br = br + self.ball_speed*brv; new_brv = brv\n",
    "        new_bc = bc + self.ball_speed*bcv; new_bcv = bcv\n",
    "        # nominal paddle state, a is action (-1, 0, 1)\n",
    "        new_pp = max(0, min(self.n-1, pp + a))\n",
    "        new_pv = a\n",
    "        new_s = None\n",
    "        # ball hit top or bot? ball hit left or the paddle (right)?\n",
    "        hit_r = hit_c = False\n",
    "        # bottom, top contacts\n",
    "        if new_br < 0:\n",
    "            new_br = 0; new_brv = 1; hit_r = True\n",
    "        elif new_br >= self.n:\n",
    "            new_br = self.n - 1; new_brv = -1; hit_r = True\n",
    "        # back, front contacts\n",
    "        if new_bc < 0:                  # back bounce\n",
    "            new_bc = 0; new_bcv = 1; hit_c = True\n",
    "        elif new_bc >= self.n:\n",
    "            if self.paddle_hit(pp, new_pp, br, bc, new_br, new_bc):\n",
    "                new_bc = self.n - 1; new_bcv = -1; hit_c = True\n",
    "            else:\n",
    "                ret_distribution['over'] = 1\n",
    "                return ret_distribution\n",
    "                #return dist.delta_dist('over')\n",
    "\n",
    "        new_s = ((new_br, new_bc), (new_brv, new_bcv), new_pp, new_pv)\n",
    "        if ((not hit_c) and (not hit_r)):\n",
    "            ret_distribution[new_s] = 1.0\n",
    "            return ret_distribution\n",
    "            #return dist.delta_dist(new_s)\n",
    "        elif hit_c:                     # also hit_c and hit_r\n",
    "            if abs(new_brv) > 0:\n",
    "                #return dist.DDist({new_s: p, ((new_br, new_bc), (-new_brv, new_bcv), new_pp, new_pv) : 1-p})\n",
    "                ret_distribution[new_s] = p\n",
    "                ret_distribution[((new_br, new_bc), (-new_brv, new_bcv), new_pp, new_pv)] = 1 - p\n",
    "                return ret_distribution\n",
    "            else:\n",
    "                ret_distribution[new_s] = p\n",
    "                ret_distribution[((new_br, new_bc), (-1, new_bcv), new_pp, new_pv)] = 0.5 * (1 - p)\n",
    "                ret_distribution[((new_br, new_bc), ( 1, new_bcv), new_pp, new_pv)] = 0.5 * (1 - p)\n",
    "                return ret_distribution\n",
    "        elif hit_r:\n",
    "            ret_distribution[new_s] = p\n",
    "            ret_distribution[((new_br, new_bc), (new_brv, -new_bcv), new_pp, new_pv)] = 1 - p\n",
    "            return ret_distribution\n",
    "\n",
    "    def paddle_hit(self, pp, new_pp, br, bc, new_br, new_bc):\n",
    "        # Being generous to paddle, any overlap in row\n",
    "        # We allow the paddle to save the ball at the very last step\n",
    "        prset = set(range(pp, pp+2)).union(set(range(new_pp, new_pp+2)))\n",
    "        brset = set([br, br+1, new_br, new_br+1])\n",
    "        return len(prset.intersection(brset)) >= 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1 Play No_Exit game with Policy Iteration (10 pts)\n",
    "Now we are going to use policy iteration to learn to play this No_Exit game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_exit_mdp = No_Exit(field_size=3)\n",
    "st_time = time.time()\n",
    "no_exit_pi = policy_iteration(no_exit_mdp, mode=\"one_step\")\n",
    "print (\"one step policy iteration time cost: \", time.time() - st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an episode (sequence of transitions) of at most\n",
    "# episode_length, using policy function to select actions.  If we find\n",
    "# a terminal state, end the episode.  Return accumulated reward a list\n",
    "# of (s, a, r, s') where s' is None for transition from terminal state.\n",
    "# Also return an animation if draw=True.\n",
    "def sim_episode(mdp, episode_length, policy, draw=False):\n",
    "    episode = []\n",
    "    reward = 0\n",
    "    s = sample(mdp.init_state)\n",
    "    all_states = [s]\n",
    "    episode_length = int(episode_length)\n",
    "    for i in range(episode_length):\n",
    "        a = policy[s]\n",
    "        (r, s_prime) = mdp.sim_transition(s, a)\n",
    "        reward += r\n",
    "        if mdp.terminal(s):\n",
    "            episode.append((s, a, r, None))\n",
    "            break\n",
    "        episode.append((s, a, r, s_prime))\n",
    "        if draw: \n",
    "            mdp.draw_state(s)\n",
    "        s = s_prime\n",
    "        all_states.append(s)\n",
    "    return reward, episode\n",
    "    #animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
    "    #return reward, episode, animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test your policy in the following ways:\n",
    "1. Set a num_episodes variable, and we let the ball move this many steps\n",
    "2. At each step, if the ball has not escaped, you get 1 reward\n",
    "3. if the ball escaped, you get 0 reward and the game ends immediately\n",
    "\n",
    "So if your policy totally learned how to play this game, you can achieve num_episodes reward for each run.\n",
    "\n",
    "We recommend you to start debug and tune parameters in a small field size (like 3), then start trying to deal with the big filed (like 6).\n",
    "\n",
    "We will finally test your algorithm in a field size 3 to 4 and episode_length from 50 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_solve_play(field_size=3, num_episodes=50, episode_length=50, mode=\"one_step\"):\n",
    "    game_mdp = No_Exit(field_size)\n",
    "    no_exit_pi = policy_iteration(game_mdp, mode=mode)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        reward, _ = sim_episode(game_mdp, episode_length, no_exit_pi)\n",
    "        rewards.append(reward)\n",
    "    print (\"Mean reward: %f +- %f\" % (np.mean(rewards), np.std(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 6mins\n",
    "st_time = time.time()\n",
    "test_solve_play(3, mode=\"exact\")\n",
    "print (\"exact policy iteration time cost: \", time.time() - st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 3mins\n",
    "st_time = time.time()\n",
    "test_solve_play(3, mode=\"one_step\")\n",
    "print (\"one_step policy iteration time cost: \", time.time() - st_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly test your policy iteration on a small 3*3 area. We can find that one_step policy iteration learns much faster. \n",
    "\n",
    "The basic reason is that at the early stage, the policy is far from the optimal one, so either exact or iterative will waste too much time on \n",
    "evaluate a very bad policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2 Learn to play the game without known the transition probability and reward function (10 pts)\n",
    "\n",
    "There are many situations where you do not know everything about the MDP. You do not know how the state transists, you do not know how the reward is assigned. But, you have a environment that you can interact with. \n",
    "\n",
    "Now you will be implementing Q-learning to learn to play the No_Exit game without accessing to the transition model and reward function. What you can do is to use **sim_transition** function to interact with the environment.\n",
    "\n",
    "To explore different actions for a specific state s, you can use either the method introduced in class or another popular one: **epsilon-greedy** which works as following:\n",
    "\n",
    "$a = \\begin{cases}\n",
    "  \\text{argmax}_{a} Q(s, a) & p > \\epsilon \\ \\text{where} \\ p \\ \\text{is a random real} \\in [0, 1) \\\\\n",
    "  \\text{random} \\ a \\in \\mathcal{A} & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "The **epsilon-greedy** method does not require the algorithm to explore each action enough times for a specific state s. When the state-action space is really huge, but your interaction time is limited, this method can help Q-learning learn a better policy.\n",
    "\n",
    "Another key factor for makeing the Q-learning work is the choice of the $\\alpha$, $N_e$ and $\\epsilon$. It should be a function of $N(s, a)$, potentially the total number of learning iterations of the Q-learning and any other related variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(mdp, max_episodes):\n",
    "    # TODO: Assignment\n",
    "    # max_episodes: the total steps you have interacted with the environment\n",
    "    #               \n",
    "    # max_step: the max step you can do in one episode, it is used to prevent\n",
    "    #           that you do infinite loop in one episode since if you learn well\n",
    "    #           the ball can never exit\n",
    "    assert False, \"Part of the Assignment\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate your Q-learning as similar as what we did for the policy iteration. \n",
    "\n",
    "We will test it with several different settings, for each setting, you are supposed to achieve around average **episode_length** reward.\n",
    "\n",
    "The given max_iteration is very generous--actually you do not need to interact that many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_learn_play(field_size=3, num_episodes=50, episode_length=50, max_episodes=100000):\n",
    "    game_mdp = No_Exit(field_size)\n",
    "    no_exit_pi = Q_learning(game_mdp, max_episodes)\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        reward, _ = sim_episode(game_mdp, episode_length, no_exit_pi)\n",
    "        rewards.append(reward)\n",
    "    print (\"Mean reward: %f +- %f\" % (np.mean(rewards), np.std(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 2mins\n",
    "st_time = time.time()\n",
    "test_learn_play(field_size=3, max_episodes=500000)\n",
    "print (\"total time for the Q-learning: \", time.time() - st_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Around 6mins\n",
    "st_time = time.time()\n",
    "test_learn_play(field_size=4, max_episodes=1000000)\n",
    "print (\"total time for the Q-learning: \", time.time() - st_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that even only for a very small field (5 * 5), the policy iteration cannot solve it in a reasonable time. \n",
    "\n",
    "The Q-learning can learn to play it a little bit (cannot learn an optimal policy) with a reasonable time.\n",
    "\n",
    "\n",
    "Below is only **For Your Interest**, you can use the method introduced below in your previous implementation to improve its efficiency.\n",
    "\n",
    "To improve the efficiency of the Q-learning, one effective way is to address its overestimation. \n",
    "\n",
    "The overestimation issue will increase the variance of the estimated expected cumulative reward. So we want to reduce these variance.\n",
    "\n",
    "There are two popular methods:\n",
    "\n",
    "1. Experience Replay Buffer\n",
    "2. Double Q-learning\n",
    "\n",
    "Their basic ideas are straightforward and you can check them out if you are interested."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d750d72901c45169fa25d5f3ca037da22f9319c06e46342da801faa15f26ee3f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
